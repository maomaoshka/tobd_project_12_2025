# tobd_project_12_2025

## краткое резюме по ролям

#### **1. Вы (DevOps / ETL Инженер):**
*   **Ваше резюме:** "сделал докер, подгрузил системы, закидываю данные в минио, собираю етл"
*   **Именно так!** Вы создаете и поддерживаете **инфраструктуру** (Docker, все сервисы). Вы реализуете шаг **Extract** (извлечение), загружая «сырые» данные в MinIO. И вы **оркестрируете** весь процесс, собирая скрипты от коллег в единый **ETL-конвейер** (Flow) с помощью Prefect.

#### **2. Инженер данных:**
*   **Ваше резюме:** "проектирует чистовую датабазу постгрес, забирает очищенные данные у 3 человека и кладет туда"
*   **Совершенно верно.** Он — архитектор **аналитического хранилища (DWH)** в PostgreSQL. Он отвечает за **финальный шаг Load** (загрузка), принимая обработанные данные от 3-го участника и загружая их в это чистое DWH.

#### **3. Инженер по обработке данных:**
*   **Ваше резюме:** "берет данные, которые я положу в минио и обрабатывает их, отдает 2 человеку"
*   **Точно.** Он — мозг всей системы. Он реализует **центральный шаг Transform** (преобразование). Он забирает «сырые» данные из MinIO, выполняет всю логику очистки и агрегации с помощью Dask и передает готовый, чистый результат 2-му участнику.

#### **4. Аналитик данных:**
*   **Ваше резюме:** "берет чистые данные с постгрес и визуализирует их"
*   **Идеально.** Он — **конечный потребитель** всей проделанной работы. Он подключается исключительно к **чистому DWH** в PostgreSQL, пишет к нему SQL-запросы и строит **дашборды**, которые превращают данные в полезные инсайты.

---

### **Визуально, ваш поток данных выглядит так:**

**1-й участник (Extract) → MinIO → 3-й участник (Transform) → [Чистые данные] → 2-й участник (Load) → PostgreSQL (DWH) → 4-й участник (Визуализация)**


#### **Шаг 0: Предварительные требования (что нужно установить)**

Перед началом убедитесь, что у вас установлено следующее программное обеспечение:

1.  **Git:** Для работы с общим кодом. -- по идее если я не сдохла, я кинула вам приглос в проект, подтвердите и лепите со мной
2.  **Docker Desktop:** **Убедитесь, что он запущен и работает** (иконка кита в трее статична) перед тем, как выполнять команды ниже.
3.  **Клиент для базы данных:** **PgAdmin** или **DBeaver** (в основном для Инженера данных и Аналитика). -- я так понимаю, мы юзаем слоняру

#### **Шаг 1: Получаем последнюю версию кода**

Откройте терминал (CMD, PowerShell, Git Bash) и выполните:

*   **Если вы еще НИ РАЗУ не скачивали проект:**
    ```bash
    git clone <URL-адрес-вашего-репозитория-на-GitHub>
    cd <название-папки-проекта>
    ```

*   **Если вы УЖЕ скачивали проект ранее:**
    ```bash
    # Перейдите в папку проекта
    cd <путь-к-вашему-проекту>
    
    # Переключитесь на основную ветку и скачайте последние изменения
    git checkout main
    git pull
    ```

#### **Шаг 2: Запускаем**

Находясь в корневой папке проекта в терминале, выполните всего одну команду:

```bash
docker-compose up -d
```

#### **Шаг 3: Проверяем, что всё работает**

После того как команда выполнится, давайте убедимся, что все сервисы "взлетели":

1.  **Проверка в терминале:**
    *   Выполните команду `docker ps`.
    *   Вы должны увидеть в списке **три** запущенных контейнера. -- вот где-то на этом шаге я наверное доползу до вас даже с опозданием, поэтому если что спрашивайте

2.  **Проверка файлового хранилища (MinIO):**
    *   Откройте браузер и перейдите по адресу: **`http://localhost:19091`** 
    *   **Логин:** `minioadmin`
    *   **Пароль:** `minioadmin`
    *   Если вы видите веб-интерфейс — всё отлично.

3.  **Проверка базы данных (PostgreSQL):**
    *   Откройте PgAdmin и создайте новое подключение со следующими параметрами:
        *   **Хост:** `localhost`
        *   **Порт:** `5432`
        *   **База данных:** `project_db`
        *   **Пользователь:** `postgres`
        *   **Пароль:** `password`
    *   Если подключение прошло успешно — вы готовы к работе!

---

### **Что дальше? Первые задачи для каждого:**

**вот тут немного разочарую товарища gemini -- у нас еще нет даже понимания, какие у нас данные, поэтому шуршим**

*   **Для Инженера данных:** Ваша первая задача — подключиться к базе данных и, используя ваш DB-клиент, создать начальную структуру таблиц (`CREATE TABLE ...`) для "сырых" данных и для будущего аналитического хранилища (DWH).
*   **Для Инженера по обработке данных:** Ваша задача — дождаться от Инженера данных примера "сырых" данных и схемы таблиц, чтобы начать писать логику их очистки и трансформации в `dask_jobs/`.
*   **Для Аналитика данных:** Ваша задача — обсудить с командой ключевые метрики, которые мы хотим видеть. Можете начать рисовать эскизы будущего дашборда и продумывать, какие SQL-запросы понадобятся для получения данных из DWH.

---

### **Если что-то пошло не так: Ошибка с портами**

Если `docker-compose up` выдает ошибку, в которой есть слова `ports are not available` или `address already in use`, это значит, что какой-то из портов на вашем компьютере уже занят.

**Решение:**
1.  Откройте файл `docker-compose.yml`.
2.  Найдите сервис, который вызвал ошибку (скорее всего, `minio`).
3.  Измените порт **слева** от двоеточия на другой (например, `19091` на `19092`).
4.  Сохраните файл и попробуйте запустить `docker-compose up -d` еще раз.

ВОТ

## щитпостинг

буду сюда кидать комментарии ллмочки пока что

### почему нет спарка
Нет, для этой работы PySpark не является обязательным, потому что преподаватель дал на выбор несколько технологий, и в качестве примера и минимальных требований указал Dask. Мы выбрали Dask, потому что это рекомендованный и более простой путь для данного проекта.

вообще ради спарка придется ковыряться с java, а я не очень хочу, поэтому вспоминаем даск

### Подключение к базе данных

После запуска `docker-compose up` база данных PostgreSQL будет доступна для подключения из любого клиента (PgAdmin для нас, наверное) со следующими параметрами:

- **Хост:** `localhost`
- **Порт:** `5432`
- **Имя базы данных:** `project_db`
- **Пользователь:** `user`
- **Пароль:** `password`

### веб интерфейс minio

 http://localhost:19091
 minioadmin / minioadmin

### **Зачем нам MinIO? ("Промышленный склад" для данных)**

Представьте, что ваш проект — это завод по производству сока.

*   **База данных PostgreSQL** — это ваш красивый, чистый **торговый зал**, где на полках стоят готовые бутылки с соком (чистые, обработанные данные). Покупатели (аналитики) приходят сюда, берут нужный сок и изучают его.
*   **Dask** — это ваш **цех по переработке**, где из фруктов делают сок (происходит обработка данных).
*   **Источник данных (API, CSV-файлы)** — это **фермеры**, которые привозят вам фрукты.

**Так вот, MinIO — это ваш центральный "промышленный склад" или "погрузочная рампа".**

Фермеры не везут тонны грязных яблок прямо в чистый торговый зал. Они сгружают их на складе. На складе яблоки лежат в больших ящиках (файлах), ждут своей очереди, и оттуда рабочие из цеха (Dask) забирают их на переработку.

**Роль MinIO в вашем проекте — это быть таким складом для "сырых" и промежуточных данных.**

1.  **Хранилище для "сырых" данных (Extract):**
    *   Первый шаг вашего ETL-конвейера (`Extract`) — это "сбор урожая". Скрипт забирает данные из внешнего источника (например, большой JSON-файл с данными биржи) и не пытается сразу запихнуть их в аккуратную табличку в PostgreSQL.
    *   Вместо этого он просто **кладет этот файл как есть** в MinIO.
    *   **Почему это хорошо?**
        *   **Скорость и простота:** Просто сохранить файл — это очень быстро.
        *   **Гибкость:** MinIO все равно, какой формат у файла — CSV, JSON, логи, картинки. Ему не нужна структура.
        *   **Надежность:** Если следующий шаг (обработка) упадет, "сырой" файл никуда не денется, и процесс можно будет запустить заново.

2.  **Источник данных для обработки (Transform):**
    *   Следующий шаг (`Transform`) — это работа цеха. Ваш скрипт на Dask (который написал Инженер по обработке данных) заходит на "склад" MinIO, берет оттуда нужный "ящик с яблоками" (файл), обрабатывает его и производит "сок" (чистый DataFrame).

3.  **Место для промежуточных данных:**
    *   Иногда в процессе обработки появляются временные, большие файлы. Их тоже удобно хранить в MinIO, чтобы не засорять базу данных.

**Как команда с ним взаимодействует:**
*   **Вы (DevOps):** Вы построили этот "склад" и следите, чтобы он работал.
*   **Инженер данных:** Он пишет код, который привозит "фрукты" от "фермеров" на ваш склад (загружает файлы в MinIO).
*   **Инженер по обработке данных:** Он пишет код, который забирает "фрукты" со склада для переработки (читает файлы из MinIO).

**Итог:** MinIO — это не база данных, а S3-совместимое **объектное хранилище**. Оно идеально подходит для хранения неструктурированных файлов любого размера, играя роль промежуточного звена в вашем конвейере данных.

### простите, у меня не стоит пгадмин

я не могу почекать, работает ли слоняра, либо посмотрим на паре, либо я скачаю его рано или поздно

### что я курила, когда выбирала порты?

почему-то в моем компе занято вообще все, а разбираться долго и сложно, поэтому сидим с 19090 

### **Что такое дашборды? ("Приборная панель" проекта)**

**Его главная цель — отвечать на вопросы бизнеса с одного взгляда.**

**Что находится на дашборде:**
*   **Графики и диаграммы:** Показывают тренды (например, как менялась цена акции за месяц).
*   **Таблицы:** Суммируют важные данные (например, топ-5 самых активных трейдеров).
*   **Ключевые показатели (KPI):** Большие цифры, которые показывают самое важное (например, "Общий объем торгов за сегодня: $1.5 млрд").
*   **Фильтры:** Интерактивные элементы (календари, выпадающие списки), которые позволяют пользователю "играть" с данными (например, посмотреть статистику только за прошлую неделю).

**Как это работает в вашем проекте:**
Это — **финальный продукт**, результат работы всей вашей команды. За него отвечает **Аналитик данных**.

1.  **Источник данных:** Дашборд **не хранит** данные. Он подключается к вашему чистому "торговому залу" — аналитическому хранилищу в **PostgreSQL (DWH)**.
2.  **Запросы:** Для каждого графика на дашборде Аналитик пишет специальный **SQL-запрос**. Например: `SELECT дата, средняя_цена FROM акции WHERE тикер='AAPL'`.
3.  **Визуализация:** Инструмент (Grafana, Plotly Dash, Streamlit) выполняет этот SQL-запрос, получает от базы данных готовую табличку с двумя колонками ("дата", "цена") и превращает ее в красивый линейный график.
4.  **Автообновление:** Дашборд может автоматически обновляться (например, каждые 5 минут), повторяя запросы и перерисовывая графики со свежими данными.

**Роль дашборда:**
Это то, что вы покажете на защите проекта. Это видимый результат всей сложной работы, которую вы с инженерами проделали "под капотом". Он доказывает, что ваша система не просто работает, а приносит пользу — позволяет анализировать данные и делать выводы.

---

## **Общие правила для всех (Остаются без изменений)**

*   **Система контроля версий (Git):**
    *   Никто не работает напрямую в ветке `main`.
    *   Каждый создает свою ветку для своей задачи (`feature/create-db-schema`, `feature/dask-transform-logic` и т.д.).
    *   Всегда делаем `git pull` перед началом работы.
    *   Завершаем работу через `Pull Request`.

---

### **1. DevOps / ETL Инженер ("Оркестратор")**

*Ваша роль остается прежней — вы создаете и поддерживаете весь "конвейер".*

*   **Шаги 1-6 (Выполнены):** Вы уже создали репозиторий, структуру, `docker-compose.yml` (с Postgres и MinIO), `Dockerfile`, `requirements.txt` и успешно все запустили. Отличная работа!

*   **Шаг 7. Написать первый Prefect Flow и реализовать `Extract`:**
    *   В папке `flows/` создайте `main_flow.py`.
    *   Опишите в нем `@flow` и три `@task`: `extract_data_to_minio()`, `transform_data_with_dask()`, `load_data_to_dwh()`.
    *   **Ваша ключевая задача:** Написать код для первой задачи, `extract_data_to_minio()`. Этот скрипт должен:
        1.  Подключиться к внешнему источнику (API или URL с файлом).
        2.  Получить "сырые" данные (например, JSON или CSV).
        3.  Подключиться к MinIO внутри Docker-сети (адрес `http://minio:9000`).
        4.  Создать `bucket` (если его нет), например, `raw-data`.
        5.  Сохранить полученные данные в виде файла в этот `bucket`.

*   **Шаг 8. Настроить запуск Prefect в Docker:**
    *   Добавьте в `docker-compose.yml` сервисы `prefect-server` и `prefect-agent`.
    *   Протестируйте запуск простого "Hello, World" flow, чтобы убедиться, что сервер и агент видят друг друга.

*   **Шаг 9. Интегрировать работу коллег:**
    *   **Дождитесь** от Инженера по обработке данных его Dask-скрипта. В вашей задаче `transform_data_with_dask()` вызовите его функцию. **Передайте ей на вход путь к файлу в MinIO**, который вы сохранили на шаге `Extract`.
    *   **Дождитесь** от Инженера данных скрипта загрузки в DWH. В задаче `load_data_to_dwh()` вызовите его функцию, **передав ей на вход результат работы Dask-скрипта**.
*   **Шаг 10. Запустить и отладить полный конвейер:**
    *   Разверните и запустите полный `flow` через интерфейс Prefect.
    *   Используйте логи для поиска и исправления ошибок вместе с командой.

*   **Шаг 11. Документация:**
    *   Финализируйте `README.md` с полными инструкциями.
    *   Напишите свою часть отчета, описывая архитектуру конвейера.

---

### **2. Инженер данных ("Архитектор DWH и Загрузчик")**

*Ваша роль сужается и становится более конкретной: вы отвечаете **только за финальное, чистое хранилище (DWH)** и за загрузку данных в него.*

*   **Шаг 1. Подготовка:** Склонировать репозиторий, запустить `docker-compose up -d`.
*   **Шаг 2. Подключиться к PostgreSQL:** Установить PgAdmin/DBeaver и подключиться к базе данных.
*   **Шаг 3. Спроектировать и создать DWH-таблицу:**
    *   **Важное изменение:** Вам **не нужно** проектировать таблицы для "сырых" данных. Эту роль выполняет MinIO.
    *   Сосредоточьтесь **только на структуре аналитического хранилища (DWH)**. Подумайте, в каком виде данные будут наиболее удобны для Аналитика.
    *   Создайте `.sql` файл (`init_db.sql`) с `CREATE TABLE` запросами только для этой финальной, чистой таблицы (или нескольких таблиц).
    *   Выполните скрипт, чтобы создать пустые таблицы в PostgreSQL.
*   **Шаг 4. Написать скрипт загрузки (`Load`):**
    *   Создайте Python-скрипт, например, `utils/load_to_dwh.py`.
    *   Напишите в нем функцию `load(dataframe)`, которая:
        1.  Принимает на вход **уже обработанный и чистый** Pandas/Dask DataFrame.
        2.  Подключается к PostgreSQL внутри Docker-сети (адрес `postgres`).
        3.  Аккуратно загружает данные из этого DataFrame в вашу DWH-таблицу (используя, например, `df.to_sql()` из SQLAlchemy).

*   **Шаг 5. Передать информацию и код:**
    *   Сообщите DevOps-инженеру, что ваш скрипт `load(dataframe)` готов.
    *   Сообщите Аналитику точные названия таблиц и полей в DWH.
    *   Закоммитьте все свои скрипты.
*   **Шаг 6. Документация:** Напишите свою часть отчета со схемой DWH и описанием процесса загрузки.

---

### **3. Инженер по обработке данных ("Мастер трансформации")**

*Ваша роль становится центральной в обработке. Вы — связующее звено между "сырым складом" MinIO и "чистым залом" DWH.*

*   **Шаг 1. Подготовка:** Склонировать репозиторий, запустить `docker-compose up -d`.
*   **Шаг 2. Получить пример данных:**
    *   **Договоритесь с DevOps-инженером**. Он положит в MinIO (`raw-data` bucket) пример "сырого" файла, с которым вам предстоит работать.
*   **Шаг 3. Разработка и отладка:**
    *   Используйте Jupyter Notebook. Напишите код, который:
        1.  Подключается к MinIO.
        2.  **Читает "сырой" файл** оттуда с помощью Dask (`dd.read_csv('s3://raw-data/myfile.csv')`).
        3.  Выполняет всю необходимую логику: очистку, фильтрацию, группировку, агрегацию.

*   **Шаг 4. Написать основной Dask-скрипт:**
    *   Перенесите отлаженную логику в Python-скрипт в папке `dask_jobs/`.
    *   Создайте главную функцию, например, `transform_and_clean(input_path_s3)`:
        1.  Она принимает на вход **путь к файлу в MinIO**.
        2.  Выполняет все трансформации с помощью Dask.
        3.  **Возвращает итоговый, чистый и обработанный DataFrame**, готовый для загрузки в DWH.
*   **Шаг 5. Передать скрипт на интеграцию:**
    *   Тщательно протестируйте скрипт. Закоммитьте его в Git.
    *   Сообщите DevOps-инженеру, что ваша функция `transform_and_clean()` готова. Будьте готовы помочь с отладкой.
*   **Шаг 6. Документация:** Напишите свою часть отчета с подробным описанием всех шагов трансформации данных.

---

### **4. Аналитик данных / Frontend-разработчик ("Визуализатор")**

*Ваша роль не меняется, но становится более зависимой от четкой работы Инженера данных.*

*   **Шаг 1. Проектирование дашборда:**
    *   **Дождитесь** от Инженера данных финальной схемы **DWH-таблиц**.
    *   Определите KPI, нарисуйте мокапы дашборда.

*   **Шаг 2. Подготовка инструментов:**
    *   Убедитесь, что можете подключиться к PostgreSQL.
    *   Подготовьте инструмент для визуализации (например, добавьте Grafana в `docker-compose.yml` вместе с DevOps-инженером).
*   **Шаг 3. Написание SQL-запросов:**
    *   Начните писать SQL-запросы к **пустым DWH-таблицам**. Проверяйте, что они синтаксически верны.

*   **Шаг 4. Создание дашборда:**
    *   **Дождитесь**, пока первый полный запуск ETL-конвейера **наполнит DWH-таблицы данными**.
    *   Подключите ваши готовые SQL-запросы к виджетам в Grafana и настройте визуализации.
*   **Шаг 5. Анализ и презентация:**
    *   Проанализируйте данные, сформулируйте выводы.
    *   Подготовьте итоговую презентацию и отчет.
