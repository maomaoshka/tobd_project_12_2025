# tobd_project_12_2025

## первый шаг с утра на паре

вообще - почитать роли в последнем блоке ридми (не призываю их придерживаться, но пока я сделала кусок от так называемого девопса - товарища 1)

а дальше погнали

#### **Шаг 0: Предварительные требования (что нужно установить)**

Перед началом убедитесь, что у вас установлено следующее программное обеспечение:

1.  **Git:** Для работы с общим кодом. -- по идее если я не сдохла, я кинула вам приглос в проект, подтвердите и лепите со мной
2.  **Docker Desktop:** **Убедитесь, что он запущен и работает** (иконка кита в трее статична) перед тем, как выполнять команды ниже.
3.  **Клиент для базы данных:** **PgAdmin** или **DBeaver** (в основном для Инженера данных и Аналитика). -- я так понимаю, мы юзаем слоняру

#### **Шаг 1: Получаем последнюю версию кода**

Откройте терминал (CMD, PowerShell, Git Bash) и выполните:

*   **Если вы еще НИ РАЗУ не скачивали проект:**
    ```bash
    git clone <URL-адрес-вашего-репозитория-на-GitHub>
    cd <название-папки-проекта>
    ```

*   **Если вы УЖЕ скачивали проект ранее:**
    ```bash
    # Перейдите в папку проекта
    cd <путь-к-вашему-проекту>
    
    # Переключитесь на основную ветку и скачайте последние изменения
    git checkout main
    git pull
    ```

#### **Шаг 2: Запускаем**

Находясь в корневой папке проекта в терминале, выполните всего одну команду:

```bash
docker-compose up -d
```

#### **Шаг 3: Проверяем, что всё работает**

После того как команда выполнится, давайте убедимся, что все сервисы "взлетели":

1.  **Проверка в терминале:**
    *   Выполните команду `docker ps`.
    *   Вы должны увидеть в списке **три** запущенных контейнера. -- вот где-то на этом шаге я наверное доползу до вас даже с опозданием, поэтому если что спрашивайте

2.  **Проверка файлового хранилища (MinIO):**
    *   Откройте браузер и перейдите по адресу: **`http://localhost:19091`** 
    *   **Логин:** `minioadmin`
    *   **Пароль:** `minioadmin`
    *   Если вы видите веб-интерфейс — всё отлично.

3.  **Проверка базы данных (PostgreSQL):**
    *   Откройте PgAdmin и создайте новое подключение со следующими параметрами:
        *   **Хост:** `localhost`
        *   **Порт:** `5432`
        *   **База данных:** `project_db`
        *   **Пользователь:** `user`
        *   **Пароль:** `password`
    *   Если подключение прошло успешно — вы готовы к работе!

---

### **Что дальше? Первые задачи для каждого:**

**вот тут немного разочарую товарища gemini -- у нас еще нет даже понимания, какие у нас данные, поэтому шуршим**

*   **Для Инженера данных:** Ваша первая задача — подключиться к базе данных и, используя ваш DB-клиент, создать начальную структуру таблиц (`CREATE TABLE ...`) для "сырых" данных и для будущего аналитического хранилища (DWH).
*   **Для Инженера по обработке данных:** Ваша задача — дождаться от Инженера данных примера "сырых" данных и схемы таблиц, чтобы начать писать логику их очистки и трансформации в `dask_jobs/`.
*   **Для Аналитика данных:** Ваша задача — обсудить с командой ключевые метрики, которые мы хотим видеть. Можете начать рисовать эскизы будущего дашборда и продумывать, какие SQL-запросы понадобятся для получения данных из DWH.

---

### **Если что-то пошло не так: Ошибка с портами**

Если `docker-compose up` выдает ошибку, в которой есть слова `ports are not available` или `address already in use`, это значит, что какой-то из портов на вашем компьютере уже занят.

**Решение:**
1.  Откройте файл `docker-compose.yml`.
2.  Найдите сервис, который вызвал ошибку (скорее всего, `minio`).
3.  Измените порт **слева** от двоеточия на другой (например, `19091` на `19092`).
4.  Сохраните файл и попробуйте запустить `docker-compose up -d` еще раз.

ВОТ

## щитпостинг

буду сюда кидать комментарии ллмочки пока что

### почему нет спарка
Нет, для этой работы PySpark не является обязательным, потому что преподаватель дал на выбор несколько технологий, и в качестве примера и минимальных требований указал Dask. Мы выбрали Dask, потому что это рекомендованный и более простой путь для данного проекта.

вообще ради спарка придется ковыряться с java, а я не очень хочу, поэтому вспоминаем даск

### Подключение к базе данных

После запуска `docker-compose up` база данных PostgreSQL будет доступна для подключения из любого клиента (PgAdmin для нас, наверное) со следующими параметрами:

- **Хост:** `localhost`
- **Порт:** `5432`
- **Имя базы данных:** `project_db`
- **Пользователь:** `user`
- **Пароль:** `password`

### веб интерфейс minio

 http://localhost:19091
 minioadmin / minioadmin

### **Зачем нам MinIO? ("Промышленный склад" для данных)**

Представьте, что ваш проект — это завод по производству сока.

*   **База данных PostgreSQL** — это ваш красивый, чистый **торговый зал**, где на полках стоят готовые бутылки с соком (чистые, обработанные данные). Покупатели (аналитики) приходят сюда, берут нужный сок и изучают его.
*   **Dask** — это ваш **цех по переработке**, где из фруктов делают сок (происходит обработка данных).
*   **Источник данных (API, CSV-файлы)** — это **фермеры**, которые привозят вам фрукты.

**Так вот, MinIO — это ваш центральный "промышленный склад" или "погрузочная рампа".**

Фермеры не везут тонны грязных яблок прямо в чистый торговый зал. Они сгружают их на складе. На складе яблоки лежат в больших ящиках (файлах), ждут своей очереди, и оттуда рабочие из цеха (Dask) забирают их на переработку.

**Роль MinIO в вашем проекте — это быть таким складом для "сырых" и промежуточных данных.**

1.  **Хранилище для "сырых" данных (Extract):**
    *   Первый шаг вашего ETL-конвейера (`Extract`) — это "сбор урожая". Скрипт забирает данные из внешнего источника (например, большой JSON-файл с данными биржи) и не пытается сразу запихнуть их в аккуратную табличку в PostgreSQL.
    *   Вместо этого он просто **кладет этот файл как есть** в MinIO.
    *   **Почему это хорошо?**
        *   **Скорость и простота:** Просто сохранить файл — это очень быстро.
        *   **Гибкость:** MinIO все равно, какой формат у файла — CSV, JSON, логи, картинки. Ему не нужна структура.
        *   **Надежность:** Если следующий шаг (обработка) упадет, "сырой" файл никуда не денется, и процесс можно будет запустить заново.

2.  **Источник данных для обработки (Transform):**
    *   Следующий шаг (`Transform`) — это работа цеха. Ваш скрипт на Dask (который написал Инженер по обработке данных) заходит на "склад" MinIO, берет оттуда нужный "ящик с яблоками" (файл), обрабатывает его и производит "сок" (чистый DataFrame).

3.  **Место для промежуточных данных:**
    *   Иногда в процессе обработки появляются временные, большие файлы. Их тоже удобно хранить в MinIO, чтобы не засорять базу данных.

**Как команда с ним взаимодействует:**
*   **Вы (DevOps):** Вы построили этот "склад" и следите, чтобы он работал.
*   **Инженер данных:** Он пишет код, который привозит "фрукты" от "фермеров" на ваш склад (загружает файлы в MinIO).
*   **Инженер по обработке данных:** Он пишет код, который забирает "фрукты" со склада для переработки (читает файлы из MinIO).

**Итог:** MinIO — это не база данных, а S3-совместимое **объектное хранилище**. Оно идеально подходит для хранения неструктурированных файлов любого размера, играя роль промежуточного звена в вашем конвейере данных.

### простите, у меня не стоит пгадмин

я не могу почекать, работает ли слоняра, либо посмотрим на паре, либо я скачаю его рано или поздно

### что я курила, когда выбирала порты?

почему-то в моем компе занято вообще все, а разбираться долго и сложно, поэтому сидим с 19090 

### **Что такое дашборды? ("Приборная панель" проекта)**

**Его главная цель — отвечать на вопросы бизнеса с одного взгляда.**

**Что находится на дашборде:**
*   **Графики и диаграммы:** Показывают тренды (например, как менялась цена акции за месяц).
*   **Таблицы:** Суммируют важные данные (например, топ-5 самых активных трейдеров).
*   **Ключевые показатели (KPI):** Большие цифры, которые показывают самое важное (например, "Общий объем торгов за сегодня: $1.5 млрд").
*   **Фильтры:** Интерактивные элементы (календари, выпадающие списки), которые позволяют пользователю "играть" с данными (например, посмотреть статистику только за прошлую неделю).

**Как это работает в вашем проекте:**
Это — **финальный продукт**, результат работы всей вашей команды. За него отвечает **Аналитик данных**.

1.  **Источник данных:** Дашборд **не хранит** данные. Он подключается к вашему чистому "торговому залу" — аналитическому хранилищу в **PostgreSQL (DWH)**.
2.  **Запросы:** Для каждого графика на дашборде Аналитик пишет специальный **SQL-запрос**. Например: `SELECT дата, средняя_цена FROM акции WHERE тикер='AAPL'`.
3.  **Визуализация:** Инструмент (Grafana, Plotly Dash, Streamlit) выполняет этот SQL-запрос, получает от базы данных готовую табличку с двумя колонками ("дата", "цена") и превращает ее в красивый линейный график.
4.  **Автообновление:** Дашборд может автоматически обновляться (например, каждые 5 минут), повторяя запросы и перерисовывая графики со свежими данными.

**Роль дашборда:**
Это то, что вы покажете на защите проекта. Это видимый результат всей сложной работы, которую вы с инженерами проделали "под капотом". Он доказывает, что ваша система не просто работает, а приносит пользу — позволяет анализировать данные и делать выводы.

## конкретный и очень подробный алгоритм действий для каждого человека в команде на четверых

### **Общие правила для всех (выполнять постоянно):**

*   **Система контроля версий (Git):**
    *   Никто не работает напрямую в ветке `main` (или `master`).
    *   Каждый участник создает свою ветку для своей задачи (например, `feature/setup-docker`, `feature/create-db-schema`).
    *   Перед началом работы всегда делайте `git pull`, чтобы забрать свежие изменения.
    *   После завершения задачи делаете `git push` и создаете Pull Request в `main` для проверки кода командой.

---

### **1 товарищ**
### **Алгоритм для: DevOps / ETL Инженера ("Оркестратор")**

Вы начинаете проект и создаете его скелет. Ваша работа — фундамент для всех остальных.

**Неделя 1: Создание инфраструктуры**

1.  **Шаг 1. Создать Git-репозиторий:**
    *   Зайдите на GitHub (или другой сервис). Создайте новый пустой репозиторий.
    *   Добавьте всех членов команды в качестве соавторов (collaborators).
    *   Склонируйте репозиторий себе на компьютер: `git clone <url>`.

2.  **Шаг 2. Создать структуру проекта:**
    *   Внутри репозитория создайте папки: `flows/`, `dask_jobs/`, `dashboards/`, `data/`.
    *   Создайте пустые файлы: `docker-compose.yml`, `Dockerfile`, `requirements.txt`, `README.md`.
    *   Сделайте первый коммит и отправьте в репозиторий: `git add .`, `git commit -m "Initial project structure"`, `git push`.

3.  **Шаг 3. Настроить `docker-compose.yml`:**
    *   Откройте `docker-compose.yml` и опишите все необходимые сервисы:
        *   `postgres`: Используйте официальный образ `postgres:14`. Задайте переменные окружения для пароля (`POSTGRES_PASSWORD`). Настройте `volumes`, чтобы данные базы не удалялись после остановки контейнера.
        *   `minio`: Используйте образ `minio/minio`. Задайте переменные для ключей доступа.
        *   `prefect-server`: Используйте образ `prefecthq/prefect:2-latest` с командой `prefect server start`.
        *   `prefect-agent`: Используйте тот же образ, но с командой `prefect agent start -q 'default'`.
    *   Пропишите для всех сервисов общую сеть (`networks`), чтобы они могли общаться.

4.  **Шаг 4. Написать `Dockerfile`:**
    *   Этот файл будет описывать образ для **вашего кода**.
    *   Начните с базового образа: `FROM python:3.9-slim`.
    *   Скопируйте `requirements.txt` и установите зависимости: `COPY requirements.txt .` и `RUN pip install -r requirements.txt`.
    *   Скопируйте весь остальной код проекта: `COPY . .`.

5.  **Шаг 5. Заполнить `requirements.txt`:**
    *   Добавьте первоначальные библиотеки: `prefect`, `dask[complete]`, `pandas`, `psycopg2-binary`, `sqlalchemy`, `s3fs`.

6.  **Шаг 6. Написать инструкции и протестировать запуск:**
    *   В `README.md` напишите, как запустить проект: `docker-compose up -d --build`.
    *   Запустите эту команду у себя. Убедитесь, что все контейнеры поднялись без ошибок (`docker ps`).
    *   Закоммитьте все изменения. **Теперь остальная команда может начинать работу.**

**Разработка и интеграция ETL**

7.  **Шаг 7. Написать скелет Prefect Flow:**
    *   В папке `flows/` создайте файл `main_flow.py`.
    *   Опишите в нем `@flow` и три пустые `@task` функции: `extract_data()`, `transform_data()`, `load_data()`.
    *   Напишите код для `extract_data()`, который берет данные из источника и сохраняет в "сырое" хранилище (MinIO/Postgres).

8.  **Шаг 8. Интегрировать работу коллег:**
    *   **Дождитесь**, когда Инженер по обработке данных предоставит свой Dask-скрипт.
    *   В задаче `transform_data()` импортируйте и вызовите его функцию.
    *   **Дождитесь**, когда Инженер данных предоставит скрипт загрузки в DWH.
    *   В задаче `load_data()` вызовите его скрипт.

9.  **Шаг 9. Отладка и тестирование:**
    *   Запустите полный флоу. Скорее всего, будут ошибки.
    *   Используйте логи в интерфейсе Prefect и `docker logs <container_name>` для отладки.
    *   Работайте вместе с другими инженерами для исправления проблем.

**Финализация**

10. **Шаг 10. Документация:**
    *   Финализируйте `README.md`, добавив полное описание проекта и шагов запуска.
    *   Напишите свою часть отчета (`report.docx`), описывая архитектуру ETL и Docker.

---

### **2 товарищ**
### **Алгоритм для: Инженера данных ("Архитектор данных")**

Вы отвечаете за хранение данных. Ваша работа тесно связана с PostgreSQL.

1.  **Шаг 1. Подготовка к работе:**
    *   **Дождитесь**, пока DevOps/ETL Инженер создаст репозиторий и базовую Docker-инфраструктуру.
    *   Склонируйте репозиторий. Запустите проект: `docker-compose up -d`.

2.  **Шаг 2. Подключиться к базе данных:**
    *   Установите клиент для работы с базами данных (DBeaver, DataGrip или pgAdmin).
    *   Создайте новое подключение к PostgreSQL. Данные для подключения возьмите из `docker-compose.yml`:
        *   Host: `localhost`
        *   Port: `5432` (или тот, что вы пробросили)
        *   User: `postgres`
        *   Password: тот, что указан в `POSTGRES_PASSWORD`.

3.  **Шаг 3. Спроектировать и создать таблицы:**
    *   На основе выбранной темы, спроектируйте две схемы таблиц:
        *   Для "сырых" данных (если они будут храниться в PG).
        *   Для аналитического хранилища (DWH), куда лягут чистые, агрегированные данные.
    *   Создайте `.sql` файл (например, `init_db.sql`) с `CREATE TABLE` запросами.
    *   Выполните этот SQL-скрипт через ваш DB-клиент, чтобы создать таблицы в базе данных внутри Docker-контейнера.

4.  **Шаг 4. Написать скрипт загрузки (Load):**
    *   Создайте Python-скрипт (например, в папке `utils/load_to_dwh.py`).
    *   Напишите в нем функцию, которая принимает на вход Pandas/Dask DataFrame и загружает его в вашу DWH-таблицу. Используйте `sqlalchemy` или `psycopg2`.

5.  **Шаг 5. Передать информацию команде:**
    *   Сообщите DevOps-инженеру, что скрипт загрузки готов и где он лежит.
    *   Сообщите Аналитику данных названия таблиц и полей в DWH, чтобы он мог писать SQL-запросы.
    *   Закоммитьте все свои скрипты в Git.

6.  **Шаг 6. Документация:**
    *   Напишите свою часть отчета, описывая структуру хранения данных и прикладывая схему БД.

---

### **3 товарищ**
### **Алгоритм для: Инженера по обработке данных ("Мастер обработки")**

Вы — мозг системы. Вы пишете логику, которая превращает сырые данные в полезную информацию.

1.  **Шаг 1. Подготовка:**
    *   **Дождитесь** DevOps-инженера и Инженера данных. Вам нужна структура проекта и понимание, в каком виде приходят "сырые" данные.
    *   Склонируйте репозиторий, запустите `docker-compose up -d`.
    *   Попросите Инженера данных предоставить вам пример "сырых" данных (например, небольшой CSV-файл).

2.  **Шаг 2. Исследование и разработка в Jupyter Notebook (опционально, но рекомендуется):**
    *   Локально (не в Docker) запустите Jupyter.
    *   Используя Pandas, загрузите пример данных и напишите код для их очистки, трансформации и агрегации. Так вы быстрее отладите логику.

3.  **Шаг 3. Написать основной Dask-скрипт:**
    *   Перенесите отлаженную логику из ноутбука в Python-скрипт в папке `dask_jobs/`.
    *   Создайте главную функцию (например, `process_data(source_path)`), которая:
        *   Читает "сырые" данные из MinIO или PG с помощью Dask.
        *   Выполняет все необходимые трансформации.
        *   Возвращает итоговый обработанный Dask DataFrame.
    *   Убедитесь, что ваш код обрабатывает данные параллельно.

4.  **Шаг 4. Передать скрипт на интеграцию:**
    *   Тщательно протестируйте свой скрипт.
    *   Закоммитьте его в Git.
    *   Сообщите DevOps-инженеру, что `transform`-компонент готов. Будьте готовы помочь ему с отладкой на этапе интеграции в Prefect.

5.  **Шаг 5. Документация:**
    *   Напишите свою часть отчета, подробно описывая все шаги обработки данных, которые вы реализовали.

---

### **4 товарищ**
### **Алгоритм для: Аналитика данных / Frontend-разработчика ("Визуализатор")**

Вы — лицо проекта. Вы представляете результат работы всей команды. Ваша основная работа начинается во второй половине проекта.

1.  **Шаг 1. Концептуальная работа (пока инженеры работают):**
    *   **Дождитесь** от Инженера данных схемы DWH-таблиц.
    *   Определите ключевые бизнес-метрики (KPI), которые вы хотите показать.
    *   Нарисуйте на бумаге или в любом редакторе эскиз (мокап) будущего дашборда. Какие графики где будут располагаться?

2.  **Шаг 2. Подготовка к работе с данными:**
    *   Установите Grafana (если она не в Docker) или дождитесь, пока DevOps добавит ее в `docker-compose.yml`.
    *   Подключитесь к DWH-таблице, используя те же данные, что и Инженер данных.

3.  **Шаг 3. Создание дашборда:**
    *   **Дождитесь**, пока первый полный запуск ETL-пайплайна заполнит DWH-таблицу данными.
    *   В Grafana (или другом инструменте) создайте новый дашборд.
    *   Для каждого графика/виджета:
        *   Напишите и отладьте SQL-запрос, который будет забирать нужные данные из DWH.
        *   Настройте тип визуализации (линейный график, гистограмма, таблица, и т.д.).

4.  **Шаг 4. Анализ и подготовка презентации:**
    *   Изучите получившийся дашборд. Какие выводы можно сделать из данных? Какие аномалии или тренды вы видите?
    *   Начните готовить презентацию проекта. Она должна включать:
        *   Описание цели и задачи.
        *   Обзор архитектуры системы (спросите у DevOps).
        *   **Демонстрацию вашего интерактивного дашборда.**
        *   **Слайды с ключевыми выводами и инсайтами, которые вы обнаружили.**

5.  **Шаг 5. Финализация проекта:**
    *   Напишите свою часть отчета, посвященную анализу данных и визуализации.
    *   Соберите финальную презентацию и проведите репетицию выступления.